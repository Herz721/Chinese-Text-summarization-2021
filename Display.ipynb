{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Display.ipynb","provenance":[],"authorship_tag":"ABX9TyOFo6NHzTkg6Y6kGBYjFOmX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pXq4lEhFCLq","executionInfo":{"status":"ok","timestamp":1620952525429,"user_tz":-480,"elapsed":23596,"user":{"displayName":"Haozhe Liu","photoUrl":"","userId":"15008509609886695232"}},"outputId":"02adbd4c-d5fe-404c-a69c-e2b277b76a23"},"source":["# 挂载Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dm7RbJytsHFQ"},"source":["# 安装bert4keras\n","! pip3 install bert4keras\n","! pip3 install gsutil\n","! gsutil cp -r gs://t5-data/pretrained_models/mt5/small .\n","! gsutil cp -r gs://t5-data/vocabs/mc4.250000.100extra/sentencepiece.model .\n","! pip3 install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQA3ioKisJZr"},"source":["# tensorflow2.x才能用GPU!!!\n","! pip3 install tensorflow==2.4.1\n","! pip3 install keras==2.3.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pahKj2x6sL6-"},"source":["# 下载Bert模型\n","! wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n","! unzip chinese_L-12_H-768_A-12.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocuia9-5sN1D"},"source":["# 解压数据集和权重\n","\n","! unzip /content/t5_in_bert4keras-main.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_a_EVqWD1ymU"},"source":["! pip3 install flask-ngrok"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9D3NsO2nFLHC","outputId":"9e5fa3aa-225b-43a4-cea8-e37a69579035"},"source":["#! -*- coding: utf-8 -*-\n","# bert做Seq2Seq任务，采用UNILM方案\n","# 介绍链接：https://kexue.fm/archives/6933\n","\n","from __future__ import print_function\n","import os\n","os.environ['TF_KERAS'] = '1'\n","import json\n","import numpy as np\n","import flask\n","from flask import Flask, request, render_template\n","from flask_ngrok import run_with_ngrok\n","from tqdm import tqdm\n","from bert4keras.backend import keras, K\n","from bert4keras.layers import Loss\n","from bert4keras.models import build_transformer_model\n","from bert4keras.tokenizers import Tokenizer, load_vocab, SpTokenizer\n","from bert4keras.optimizers import Adam\n","from bert4keras.snippets import sequence_padding, open\n","from bert4keras.snippets import DataGenerator, AutoRegressiveDecoder\n","from keras.models import Model\n","\n","# t5基本参数\n","max_c_len = 256\n","# UniLM基本参数\n","maxlen = 256\n","\n","# bert配置\n","UniLM_config_path = '/content/chinese_L-12_H-768_A-12/bert_config.json'\n","UniLM_checkpoint_path = '/content/chinese_L-12_H-768_A-12/bert_model.ckpt'\n","dict_path = '/content/chinese_L-12_H-768_A-12/vocab.txt'\n","# t5配置\n","mt5_config_path = '/content/small/t5_small.json'\n","mt5_checkpoint_path = '/content/small/model.ckpt-1000000'\n","mt5_spm_path = '/content/t5_in_bert4keras-main/tokenizer/sentencepiece_cn.model'\n","mt5_keep_tokens_path = '/content/t5_in_bert4keras-main/tokenizer/sentencepiece_cn_keep_tokens.json'\n","\n","# 加载t5分词器\n","mt5_tokenizer = SpTokenizer(mt5_spm_path, token_start=None, token_end='</s>')\n","mt5_keep_tokens = json.load(open(mt5_keep_tokens_path))\n","# 加载UniLM分词器\n","token_dict, UniLM_keep_tokens = load_vocab(\n","    dict_path=dict_path,\n","    simplified=True,\n","    startswith=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n",")\n","UniLM_tokenizer = Tokenizer(token_dict, do_lower_case=True)\n","\n","class mt5_CrossEntropy(Loss):\n","    \"\"\"交叉熵作为loss，并mask掉输入部分\n","    \"\"\"\n","    def compute_loss(self, inputs, mask=None):\n","        y_true, y_pred = inputs\n","        y_true = y_true[:, 1:]  # 目标token_ids\n","        y_mask = K.cast(mask[1], K.floatx())[:, :-1]  # 解码器自带mask\n","        y_pred = y_pred[:, :-1]  # 预测序列，错开一位\n","        loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n","        loss = K.sum(loss * y_mask) / K.sum(y_mask)\n","        return loss\n","# mt5_model配置\n","t5 = build_transformer_model(\n","    config_path=mt5_config_path,\n","    checkpoint_path=mt5_checkpoint_path,\n","    keep_tokens=mt5_keep_tokens,\n","    model='t5.1.1',\n","    return_keras_model=False,\n","    name='T5',\n",")\n","encoder = t5.encoder\n","decoder = t5.decoder\n","mt5_model = t5.model\n","mt5_model.summary()\n","output = mt5_CrossEntropy(1)([mt5_model.inputs[1], mt5_model.outputs[0]])\n","mt5_model = Model(mt5_model.inputs, output)\n","mt5_model.compile(optimizer=Adam(2e-4))\n","\n","class UniLM_CrossEntropy(Loss):\n","    \"\"\"交叉熵作为loss，并mask掉输入部分\n","    \"\"\"\n","    def compute_loss(self, inputs, mask=None):\n","        y_true, y_mask, y_pred = inputs\n","        y_true = y_true[:, 1:]  # 目标token_ids\n","        y_mask = y_mask[:, 1:]  # segment_ids，刚好指示了要预测的部分\n","        y_pred = y_pred[:, :-1]  # 预测序列，错开一位\n","        loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n","        loss = K.sum(loss * y_mask) / K.sum(y_mask)\n","        return loss\n","\n","# UniLM_model配置\n","UniLM_model = build_transformer_model(\n","    UniLM_config_path, # 模型的配置文件\n","    UniLM_checkpoint_path, # 模型的预训练权重\n","    application='unilm', # 模型的用途\n","    keep_tokens=UniLM_keep_tokens,  # 只保留keep_tokens中的字，精简原字表\n",")\n","output = UniLM_CrossEntropy(2)(UniLM_model.inputs + UniLM_model.outputs)\n","UniLM_model = Model(UniLM_model.inputs, output)\n","UniLM_model.compile(optimizer=Adam(1e-5))\n","UniLM_model.summary()\n","\n","# flask展示\n","app = Flask(__name__)\n","run_with_ngrok(app)\n","\n","class mt5_AutoTitle(AutoRegressiveDecoder):\n","    \"\"\"seq2seq解码器\n","    \"\"\"\n","    @AutoRegressiveDecoder.wraps(default_rtype='probas')\n","    def predict(self, inputs, output_ids, states):\n","        c_encoded = inputs[0]\n","        return decoder.predict([c_encoded, output_ids])[:, -1]\n","\n","    def generate(self, text, topk=1):\n","        c_token_ids, _ = mt5_tokenizer.encode(text, maxlen=max_c_len)\n","        c_encoded = encoder.predict(np.array([c_token_ids]))[0]\n","        output_ids = self.beam_search([c_encoded], topk)  # 基于beam search\n","        return mt5_tokenizer.decode([int(i) for i in output_ids])\n","# T5有一个很让人不解的设置，它的<bos>标记id是0，即<bos>和<pad>其实都是0\n","mt5_autotitle = mt5_AutoTitle(start_id=0, end_id=mt5_tokenizer._token_end_id, maxlen=128)\n","\n","class UniLM_AutoTitle(AutoRegressiveDecoder):\n","    \"\"\"seq2seq解码器\n","    \"\"\"\n","    @AutoRegressiveDecoder.wraps(default_rtype='probas')\n","    def predict(self, inputs, output_ids, states):\n","        token_ids, segment_ids = inputs\n","        token_ids = np.concatenate([token_ids, output_ids], 1)\n","        segment_ids = np.concatenate([segment_ids, np.ones_like(output_ids)], 1)\n","        return self.last_token(UniLM_model).predict([token_ids, segment_ids])\n","\n","    def generate(self, text, topk=1):\n","        max_c_len = maxlen - self.maxlen\n","        token_ids, segment_ids = UniLM_tokenizer.encode(text, maxlen=max_c_len)\n","        output_ids = self.beam_search([token_ids, segment_ids],\n","                                      topk=topk)  # 基于beam search\n","        return UniLM_tokenizer.decode(output_ids)\n","# UniLM设置\n","UniLM_autotitle = UniLM_AutoTitle(start_id=None, end_id=UniLM_tokenizer._token_end_id, maxlen=128)\n","\n","@app.route('/')\n","def index():\n","    return render_template('index.html')\n","\n","\n","@app.route('/predict', methods=['POST'])\n","def predict():\n","    try:\n","        sentence = request.json['input_text']\n","        model = request.json['model']\n","        if sentence != '':\n","            if model.lower() == 'unilm':\n","                output = UniLM_autotitle.generate(sentence)\n","            else:\n","                output = mt5_autotitle.generate(sentence)\n","            response = {}\n","            response['response'] = {\n","                'summary': str(output),\n","                'model': model.lower()\n","            }\n","            return flask.jsonify(response)\n","        else:\n","            res = dict({'message': 'Empty input'})\n","            return app.response_class(response=json.dumps(res), status=500, mimetype='application/json')\n","    except Exception as ex:\n","        res = dict({'message': str(ex)})\n","        print(res)\n","        return app.response_class(response=json.dumps(res), status=500, mimetype='application/json')\n","\n","if __name__ == '__main__':\n","    UniLM_model.load_weights('./gdrive/MyDrive/UniLM_Bert4Keras/best_model.weights')\n","    mt5_model.load_weights('./gdrive/MyDrive/T5_Bert4Keras/best_model.weights')\n","    app.run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Encoder-Input-Token (InputLayer [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","Embedding-Token (Embedding)     (None, None, 512)    16690176    Encoder-Input-Token[0][0]        \n","__________________________________________________________________________________________________\n","Encoder-Embedding-Dropout (Drop (None, None, 512)    0           Embedding-Token[0][0]            \n","__________________________________________________________________________________________________\n","Encoder-Transformer-0-MultiHead (None, None, 512)    512         Encoder-Embedding-Dropout[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-Embedding-Relative-Posi (None, None, 6)      192         Encoder-Embedding-Dropout[0][0]  \n","                                                                 Encoder-Embedding-Dropout[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-Transformer-0-MultiHead (None, None, 512)    786432      Encoder-Transformer-0-MultiHeadSe\n","                                                                 Encoder-Transformer-0-MultiHeadSe\n","                                                                 Encoder-Transformer-0-MultiHeadSe\n","                                                                 Encoder-Embedding-Relative-Positi\n","__________________________________________________________________________________________________\n","Encoder-Transformer-0-MultiHead (None, None, 512)    0           Encoder-Transformer-0-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-0-MultiHead (None, None, 512)    0           Encoder-Embedding-Dropout[0][0]  \n","                                                                 Encoder-Transformer-0-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-0-FeedForwa (None, None, 512)    512         Encoder-Transformer-0-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-0-FeedForwa (None, None, 512)    1572864     Encoder-Transformer-0-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-0-FeedForwa (None, None, 512)    0           Encoder-Transformer-0-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-0-FeedForwa (None, None, 512)    0           Encoder-Transformer-0-MultiHeadSe\n","                                                                 Encoder-Transformer-0-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-1-MultiHead (None, None, 512)    512         Encoder-Transformer-0-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-1-MultiHead (None, None, 512)    786432      Encoder-Transformer-1-MultiHeadSe\n","                                                                 Encoder-Transformer-1-MultiHeadSe\n","                                                                 Encoder-Transformer-1-MultiHeadSe\n","                                                                 Encoder-Embedding-Relative-Positi\n","__________________________________________________________________________________________________\n","Encoder-Transformer-1-MultiHead (None, None, 512)    0           Encoder-Transformer-1-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-1-MultiHead (None, None, 512)    0           Encoder-Transformer-0-FeedForward\n","                                                                 Encoder-Transformer-1-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-1-FeedForwa (None, None, 512)    512         Encoder-Transformer-1-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-1-FeedForwa (None, None, 512)    1572864     Encoder-Transformer-1-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-1-FeedForwa (None, None, 512)    0           Encoder-Transformer-1-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-1-FeedForwa (None, None, 512)    0           Encoder-Transformer-1-MultiHeadSe\n","                                                                 Encoder-Transformer-1-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-2-MultiHead (None, None, 512)    512         Encoder-Transformer-1-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-2-MultiHead (None, None, 512)    786432      Encoder-Transformer-2-MultiHeadSe\n","                                                                 Encoder-Transformer-2-MultiHeadSe\n","                                                                 Encoder-Transformer-2-MultiHeadSe\n","                                                                 Encoder-Embedding-Relative-Positi\n","__________________________________________________________________________________________________\n","Encoder-Transformer-2-MultiHead (None, None, 512)    0           Encoder-Transformer-2-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-2-MultiHead (None, None, 512)    0           Encoder-Transformer-1-FeedForward\n","                                                                 Encoder-Transformer-2-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-2-FeedForwa (None, None, 512)    512         Encoder-Transformer-2-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-2-FeedForwa (None, None, 512)    1572864     Encoder-Transformer-2-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-2-FeedForwa (None, None, 512)    0           Encoder-Transformer-2-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-2-FeedForwa (None, None, 512)    0           Encoder-Transformer-2-MultiHeadSe\n","                                                                 Encoder-Transformer-2-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-3-MultiHead (None, None, 512)    512         Encoder-Transformer-2-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-3-MultiHead (None, None, 512)    786432      Encoder-Transformer-3-MultiHeadSe\n","                                                                 Encoder-Transformer-3-MultiHeadSe\n","                                                                 Encoder-Transformer-3-MultiHeadSe\n","                                                                 Encoder-Embedding-Relative-Positi\n","__________________________________________________________________________________________________\n","Encoder-Transformer-3-MultiHead (None, None, 512)    0           Encoder-Transformer-3-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-3-MultiHead (None, None, 512)    0           Encoder-Transformer-2-FeedForward\n","                                                                 Encoder-Transformer-3-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-3-FeedForwa (None, None, 512)    512         Encoder-Transformer-3-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-3-FeedForwa (None, None, 512)    1572864     Encoder-Transformer-3-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-3-FeedForwa (None, None, 512)    0           Encoder-Transformer-3-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-3-FeedForwa (None, None, 512)    0           Encoder-Transformer-3-MultiHeadSe\n","                                                                 Encoder-Transformer-3-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-4-MultiHead (None, None, 512)    512         Encoder-Transformer-3-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-4-MultiHead (None, None, 512)    786432      Encoder-Transformer-4-MultiHeadSe\n","                                                                 Encoder-Transformer-4-MultiHeadSe\n","                                                                 Encoder-Transformer-4-MultiHeadSe\n","                                                                 Encoder-Embedding-Relative-Positi\n","__________________________________________________________________________________________________\n","Encoder-Transformer-4-MultiHead (None, None, 512)    0           Encoder-Transformer-4-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-4-MultiHead (None, None, 512)    0           Encoder-Transformer-3-FeedForward\n","                                                                 Encoder-Transformer-4-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-4-FeedForwa (None, None, 512)    512         Encoder-Transformer-4-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-4-FeedForwa (None, None, 512)    1572864     Encoder-Transformer-4-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-4-FeedForwa (None, None, 512)    0           Encoder-Transformer-4-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-4-FeedForwa (None, None, 512)    0           Encoder-Transformer-4-MultiHeadSe\n","                                                                 Encoder-Transformer-4-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-5-MultiHead (None, None, 512)    512         Encoder-Transformer-4-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-5-MultiHead (None, None, 512)    786432      Encoder-Transformer-5-MultiHeadSe\n","                                                                 Encoder-Transformer-5-MultiHeadSe\n","                                                                 Encoder-Transformer-5-MultiHeadSe\n","                                                                 Encoder-Embedding-Relative-Positi\n","__________________________________________________________________________________________________\n","Encoder-Transformer-5-MultiHead (None, None, 512)    0           Encoder-Transformer-5-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-5-MultiHead (None, None, 512)    0           Encoder-Transformer-4-FeedForward\n","                                                                 Encoder-Transformer-5-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-5-FeedForwa (None, None, 512)    512         Encoder-Transformer-5-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-5-FeedForwa (None, None, 512)    1572864     Encoder-Transformer-5-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-5-FeedForwa (None, None, 512)    0           Encoder-Transformer-5-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-5-FeedForwa (None, None, 512)    0           Encoder-Transformer-5-MultiHeadSe\n","                                                                 Encoder-Transformer-5-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-6-MultiHead (None, None, 512)    512         Encoder-Transformer-5-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-6-MultiHead (None, None, 512)    786432      Encoder-Transformer-6-MultiHeadSe\n","                                                                 Encoder-Transformer-6-MultiHeadSe\n","                                                                 Encoder-Transformer-6-MultiHeadSe\n","                                                                 Encoder-Embedding-Relative-Positi\n","__________________________________________________________________________________________________\n","Encoder-Transformer-6-MultiHead (None, None, 512)    0           Encoder-Transformer-6-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-6-MultiHead (None, None, 512)    0           Encoder-Transformer-5-FeedForward\n","                                                                 Encoder-Transformer-6-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-6-FeedForwa (None, None, 512)    512         Encoder-Transformer-6-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-6-FeedForwa (None, None, 512)    1572864     Encoder-Transformer-6-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-6-FeedForwa (None, None, 512)    0           Encoder-Transformer-6-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-6-FeedForwa (None, None, 512)    0           Encoder-Transformer-6-MultiHeadSe\n","                                                                 Encoder-Transformer-6-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-7-MultiHead (None, None, 512)    512         Encoder-Transformer-6-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-7-MultiHead (None, None, 512)    786432      Encoder-Transformer-7-MultiHeadSe\n","                                                                 Encoder-Transformer-7-MultiHeadSe\n","                                                                 Encoder-Transformer-7-MultiHeadSe\n","                                                                 Encoder-Embedding-Relative-Positi\n","__________________________________________________________________________________________________\n","Encoder-Transformer-7-MultiHead (None, None, 512)    0           Encoder-Transformer-7-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-7-MultiHead (None, None, 512)    0           Encoder-Transformer-6-FeedForward\n","                                                                 Encoder-Transformer-7-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-7-FeedForwa (None, None, 512)    512         Encoder-Transformer-7-MultiHeadSe\n","__________________________________________________________________________________________________\n","Encoder-Transformer-7-FeedForwa (None, None, 512)    1572864     Encoder-Transformer-7-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-7-FeedForwa (None, None, 512)    0           Encoder-Transformer-7-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Transformer-7-FeedForwa (None, None, 512)    0           Encoder-Transformer-7-MultiHeadSe\n","                                                                 Encoder-Transformer-7-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Output-Norm (LayerNorma (None, None, 512)    512         Encoder-Transformer-7-FeedForward\n","__________________________________________________________________________________________________\n","Encoder-Output-Dropout (Dropout (None, None, 512)    0           Encoder-Output-Norm[0][0]        \n","__________________________________________________________________________________________________\n","Decoder-Input-Token (InputLayer [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","T5_Decoder (Functional)         (None, None, 32598)  58559168    Encoder-Output-Dropout[0][0]     \n","                                                                 Decoder-Input-Token[0][0]        \n","==================================================================================================\n","Total params: 77,442,432\n","Trainable params: 77,442,432\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Input-Token (InputLayer)        [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","Input-Segment (InputLayer)      [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","Embedding-Token (Embedding)     multiple             10432512    Input-Token[0][0]                \n","                                                                 MLM-Norm[0][0]                   \n","__________________________________________________________________________________________________\n","Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n","__________________________________________________________________________________________________\n","Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n","                                                                 Embedding-Segment[0][0]          \n","__________________________________________________________________________________________________\n","Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n","__________________________________________________________________________________________________\n","Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n","__________________________________________________________________________________________________\n","Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n","__________________________________________________________________________________________________\n","Attention-UniLM-Mask (Lambda)   (None, 1, None, None 0           Input-Segment[0][0]              \n","__________________________________________________________________________________________________\n","Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n","                                                                 Embedding-Dropout[0][0]          \n","                                                                 Embedding-Dropout[0][0]          \n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n","                                                                 Transformer-0-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n","                                                                 Transformer-0-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n","                                                                 Transformer-0-FeedForward-Norm[0]\n","                                                                 Transformer-0-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n","                                                                 Transformer-1-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n","                                                                 Transformer-1-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n","                                                                 Transformer-1-FeedForward-Norm[0]\n","                                                                 Transformer-1-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n","                                                                 Transformer-2-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n","                                                                 Transformer-2-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n","                                                                 Transformer-2-FeedForward-Norm[0]\n","                                                                 Transformer-2-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n","                                                                 Transformer-3-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n","                                                                 Transformer-3-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n","                                                                 Transformer-3-FeedForward-Norm[0]\n","                                                                 Transformer-3-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n","                                                                 Transformer-4-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n","                                                                 Transformer-4-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n","                                                                 Transformer-4-FeedForward-Norm[0]\n","                                                                 Transformer-4-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n","                                                                 Transformer-5-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n","                                                                 Transformer-5-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n","                                                                 Transformer-5-FeedForward-Norm[0]\n","                                                                 Transformer-5-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n","                                                                 Transformer-6-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n","                                                                 Transformer-6-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n","                                                                 Transformer-6-FeedForward-Norm[0]\n","                                                                 Transformer-6-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n","                                                                 Transformer-7-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n","                                                                 Transformer-7-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n","                                                                 Transformer-7-FeedForward-Norm[0]\n","                                                                 Transformer-7-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n","                                                                 Transformer-8-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n","                                                                 Transformer-8-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n","                                                                 Transformer-8-FeedForward-Norm[0]\n","                                                                 Transformer-8-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n","                                                                 Transformer-9-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n","__________________________________________________________________________________________________\n","Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n","__________________________________________________________________________________________________\n","Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n","                                                                 Transformer-9-FeedForward-Dropout\n","__________________________________________________________________________________________________\n","Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n","__________________________________________________________________________________________________\n","Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n","                                                                 Transformer-9-FeedForward-Norm[0]\n","                                                                 Transformer-9-FeedForward-Norm[0]\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n","__________________________________________________________________________________________________\n","Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n","                                                                 Transformer-10-MultiHeadSelfAtten\n","__________________________________________________________________________________________________\n","Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n","__________________________________________________________________________________________________\n","Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n","__________________________________________________________________________________________________\n","Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n","__________________________________________________________________________________________________\n","Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n","                                                                 Transformer-10-FeedForward-Dropou\n","__________________________________________________________________________________________________\n","Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n","__________________________________________________________________________________________________\n","Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n","                                                                 Transformer-10-FeedForward-Norm[0\n","                                                                 Transformer-10-FeedForward-Norm[0\n","                                                                 Attention-UniLM-Mask[0][0]       \n","__________________________________________________________________________________________________\n","Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n","__________________________________________________________________________________________________\n","Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n","                                                                 Transformer-11-MultiHeadSelfAtten\n","__________________________________________________________________________________________________\n","Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n","__________________________________________________________________________________________________\n","Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n","__________________________________________________________________________________________________\n","Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n","__________________________________________________________________________________________________\n","Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n","                                                                 Transformer-11-FeedForward-Dropou\n","__________________________________________________________________________________________________\n","Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n","__________________________________________________________________________________________________\n","MLM-Dense (Dense)               (None, None, 768)    590592      Transformer-11-FeedForward-Norm[0\n","__________________________________________________________________________________________________\n","MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n","__________________________________________________________________________________________________\n","MLM-Bias (BiasAdd)              (None, None, 13584)  13584       Embedding-Token[1][0]            \n","__________________________________________________________________________________________________\n","MLM-Activation (Activation)     (None, None, 13584)  0           MLM-Bias[0][0]                   \n","__________________________________________________________________________________________________\n","uni_lm__cross_entropy (UniLM_Cr (None, None, 13584)  0           Input-Token[0][0]                \n","                                                                 Input-Segment[0][0]              \n","                                                                 MLM-Activation[0][0]             \n","==================================================================================================\n","Total params: 96,488,976\n","Trainable params: 96,488,976\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"," * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"],"name":"stdout"},{"output_type":"stream","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"],"name":"stderr"},{"output_type":"stream","text":[" * Running on http://2c05544be289.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [14/May/2021 00:46:35] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/js/jquery-3.4.1.min.js HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/js/bootstrap.min.js HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/css/app.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/js/app.js HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/css/app.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/js/app.js HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/js/jquery-3.4.1.min.js HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/js/bootstrap.min.js HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:36] \"\u001b[37mGET /static/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:46:37] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [14/May/2021 00:47:29] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:48:07] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:50:14] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 126 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f14558894d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [14/May/2021 00:51:09] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:52:24] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 00:57:09] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:00:09] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:01:07] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:01:37] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:03:21] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:03:30] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:04:31] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:05:02] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:05:50] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:11:23] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:13:11] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [14/May/2021 01:13:43] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"}]}]}